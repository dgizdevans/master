{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "k3aDCc7-SGKk",
        "DXTqRKmiq1rk"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 4: Classification"
      ],
      "metadata": {
        "id": "tcOyeSk-wWfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this lab, we will perform a classification analysis using a dataset that was cleaned and labeled in Computer Practice 2. Our goal is to classify the data into two distinct classes based on the labels from the previous clustering analysis. We will use various classification techniques, including discriminant analysis and logistic regression, to evaluate the performance of our models. This process will involve splitting the data, scaling features, and assessing the models using several metrics to determine their effectiveness. By the end of this lab, we will have a better understanding of how well our classifiers perform and which features are most important for classification."
      ],
      "metadata": {
        "id": "dhHShrKyQ-p-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries\n",
        "\n",
        "In this section, we will import the essential libraries required for data manipulation, visualization, and machine learning. These libraries will help us handle the dataset, create visualizations, perform discriminant analysis, and evaluate our classification models.\n",
        "\n"
      ],
      "metadata": {
        "id": "pWkfK8rCwt5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries for data manipulation and analysis\n",
        "import pandas as pd  # For handling data structures and operations\n",
        "import numpy as np   # For numerical operations\n",
        "\n",
        "# Importing libraries for data visualization\n",
        "import matplotlib.pyplot as plt  # For creating static, animated, and interactive visualizations\n",
        "import seaborn as sns  # For statistical data visualization\n",
        "\n",
        "# Importing libraries for machine learning and classification\n",
        "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
        "from sklearn.preprocessing import StandardScaler  # For feature scaling\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA # For discriminant analysis\n",
        "from sklearn.linear_model import LogisticRegression  # For logistic regression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score  # For evaluating classification models\n",
        "\n",
        "# Importing libraries for statistical analysis\n",
        "import statsmodels.api as sm  # For statistical modeling and testing\n",
        "from scipy.stats import chi2_contingency  # For Chi-square test\n"
      ],
      "metadata": {
        "id": "eNlaPgXUwxtS"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## Load and Inspect Data\n",
        "\n",
        "The first step in any data analysis process is to load and inspect the dataset. This essential phase ensures we have a solid understanding of the data's structure, the type of information it contains, and any potential issues that need addressing before deep analysis. This step helps in identifying the right tools and methods for subsequent data manipulation and analysis, ensuring the reliability and accuracy of our findings."
      ],
      "metadata": {
        "id": "eykVelN4w14Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and Load Dataset\n",
        "\n",
        "To start our analysis, we first need to access the data. Download the dataset from a URL and load it into a pandas DataFrame."
      ],
      "metadata": {
        "id": "k3aDCc7-SGKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset from the specified URL and save it as 'dataset1.csv'\n",
        "!wget -O dataset4.csv https://tsidmlab.fra1.digitaloceanspaces.com/df_w_labels_2.csv\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv('dataset4.csv')"
      ],
      "metadata": {
        "id": "9R3a0eXYw5xJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7028c5e2-3edf-4cbf-8e04-4c8ac3307190"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-13 21:54:59--  https://tsidmlab.fra1.digitaloceanspaces.com/df_w_labels_2.csv\n",
            "Resolving tsidmlab.fra1.digitaloceanspaces.com (tsidmlab.fra1.digitaloceanspaces.com)... 5.101.109.44\n",
            "Connecting to tsidmlab.fra1.digitaloceanspaces.com (tsidmlab.fra1.digitaloceanspaces.com)|5.101.109.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14552 (14K) [application/vnd.ms-excel]\n",
            "Saving to: ‘dataset4.csv’\n",
            "\n",
            "dataset4.csv        100%[===================>]  14.21K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-13 21:55:00 (198 MB/s) - ‘dataset4.csv’ saved [14552/14552]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qucik Inspect Data\n",
        "\n",
        "In this section, we will load the dataset previously cleaned and prepared in Lab 2. As we have already performed extensive cleaning and preprocessing, this step will be straightforward. We will download the dataset, make a quick overview to ensure everything is in order, and proceed to the next steps. This approach allows us to focus more on the classification tasks ahead.\n",
        "\n"
      ],
      "metadata": {
        "id": "iFjVGDJmP0Iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To quickly review the structure and content of our dataset, we will display the first 5 rows.\n"
      ],
      "metadata": {
        "id": "Y3-BXF5FxtBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Print the first 5 rows of the DataFrame to get an overview of the data\n",
        " print(df.head())"
      ],
      "metadata": {
        "id": "iLWf7ng29GPf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "decaefe3-5d6e-4841-d368-5017f63f86a3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   housing_median_age  total_rooms  total_bedrooms  population  households  \\\n",
            "0                28.0       1343.0           330.0      1331.0       305.0   \n",
            "1                14.0       6788.0          1216.0      2866.0      1036.0   \n",
            "2                40.0       1718.0           391.0      1312.0       388.0   \n",
            "3                16.0       2947.0           802.0      1385.0       743.0   \n",
            "4                24.0       1602.0           426.0       751.0       257.0   \n",
            "\n",
            "   median_income  median_house_value  labels  \n",
            "0         1.5160             56700.0       1  \n",
            "1         3.3603            280200.0       0  \n",
            "2         2.9955            134700.0       1  \n",
            "3         3.6731            318000.0       1  \n",
            "4         1.7609             99300.0       1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first 5 rows of our dataset provide a quick snapshot of the data we will be using for classification. Each row represents a unique data point with various features such as housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value, and the class labels. This initial overview confirms that the dataset includes both feature variables and the target labels, making it suitable for classification analysis.\n"
      ],
      "metadata": {
        "id": "ojDdB5O0x42A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimensions of the DataFrame**\n",
        "\n",
        "To understand the overall size of our dataset, we examine its dimensions."
      ],
      "metadata": {
        "id": "WuRfN6hST4hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dimensions of the DataFrame (number of rows and columns)\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "CTB4fuX29ZTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f79b24e-a53f-44b9-c5e4-b6642d573fcd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DataFrame contains 300 rows and 8 columns, indicating a dataset with 300 observations and 8 attributes including the class labels."
      ],
      "metadata": {
        "id": "O-fuY89ZT6Xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Types of Each Column**\n",
        "\n",
        "Understanding the data types helps in identifying any necessary type conversions and in choosing appropriate analysis methods."
      ],
      "metadata": {
        "id": "iJloTagD2FGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the data types of each column in the DataFrame to understand the types of data being dealt with\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "id": "5Svvo2c29f2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb9bcfe2-ad06-446a-c67b-c4efe9541a49"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "housing_median_age    float64\n",
            "total_rooms           float64\n",
            "total_bedrooms        float64\n",
            "population            float64\n",
            "households            float64\n",
            "median_income         float64\n",
            "median_house_value    float64\n",
            "labels                  int64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data types are as expected: most columns are of type float64, except for the labels column, which is of type int64. This confirms that our dataset is suitable for numerical analysis and classification."
      ],
      "metadata": {
        "id": "J77vqMcb3Tep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Descriptive Statistics**\n",
        "\n",
        "Generating descriptive statistics helps summarize the central tendency, dispersion, and shape of the dataset's distribution, providing a basic understanding of each feature."
      ],
      "metadata": {
        "id": "yoGZbcQ55Ach"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate descriptive statistics that summarize the central tendency,\n",
        "# dispersion, and shape of the dataset's distribution, excluding NaN values.\n",
        "# Transpose the result for better readability and round the numbers to two decimal places.\n",
        "descriptive_stats = df.describe().T.round(2)\n",
        "\n",
        "# Print the descriptive statistics for the dataset\n",
        "print(descriptive_stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO6Evfs6-lQI",
        "outputId": "657472d7-9d07-42c9-8fd0-bdb8170d48e0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    count       mean        std       min        25%  \\\n",
            "housing_median_age  300.0      29.10      12.27      2.00      18.75   \n",
            "total_rooms         300.0    2522.21    1858.79    123.00    1475.75   \n",
            "total_bedrooms      300.0     521.90     407.82     28.00     306.00   \n",
            "population          300.0    1388.02    1009.12     88.00     818.75   \n",
            "households          300.0     488.17     382.90     26.00     286.50   \n",
            "median_income       300.0       3.90       1.98      0.64       2.46   \n",
            "median_house_value  300.0  215246.06  122631.87  49500.00  123400.00   \n",
            "labels              300.0       0.85       0.36      0.00       1.00   \n",
            "\n",
            "                          50%        75%       max  \n",
            "housing_median_age      30.00      37.25      52.0  \n",
            "total_rooms           2031.50    2947.00   17377.0  \n",
            "total_bedrooms         408.00     632.50    4457.0  \n",
            "population            1148.50    1694.00    7817.0  \n",
            "households             376.00     595.75    4204.0  \n",
            "median_income            3.54       4.84      15.0  \n",
            "median_house_value  181150.00  275000.00  500001.0  \n",
            "labels                   1.00       1.00       1.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The descriptive statistics show the count, mean, standard deviation, minimum, 25th percentile, median, 75th percentile, and maximum for each variable. This gives us a detailed overview of the distribution and spread of the data, highlighting potential areas for further analysis, such as the wide range in total_rooms and median_house_value."
      ],
      "metadata": {
        "id": "78iHKEX1UViR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion on Quick Data Inspection**\n",
        "\n",
        "> The quick inspection of the dataset confirms that it is well-prepared for the upcoming classification tasks. The data includes 300 observations with 8 attributes, including the class labels. All features are numerical, which is appropriate for discriminant analysis and logistic regression. The descriptive statistics provide a solid overview of the data distribution, confirming that the dataset is suitable for further analysis without any immediate need for additional cleaning or transformation. We can now confidently proceed with the next steps in our classification analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LVb_9Yv6UWuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation for Classification\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tdsj_6JzSvE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the Dataset into Training and Testing Sets\n",
        "\n",
        "To effectively evaluate the performance of our classification model, we need to split the dataset into training and testing sets. The training set will be used to build the model, while the testing set will be used to assess its accuracy and generalizability."
      ],
      "metadata": {
        "id": "g7zEA4i5lP-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the feature set (X) and the target variable (y)\n",
        "X = df.drop('labels', axis=1)\n",
        "y = df['labels']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# We use 80% of the data for training and 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting datasets\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")\n",
        "print(f\"Testing labels shape: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5es1zAY8WmuM",
        "outputId": "f5153bf5-aa83-463c-85fe-6771c7041622"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (240, 7)\n",
            "Testing set shape: (60, 7)\n",
            "Training labels shape: (240,)\n",
            "Testing labels shape: (60,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has been successfully split into training and testing sets. Here are the details of the split:\n",
        "\n",
        "- **Training set shape: (240, 7)**: This indicates that the training set consists of 240 samples with 7 features each. These features will be used to train the classification model.\n",
        "\n",
        "- **Testing set shape: (60, 7)**: The testing set contains 60 samples with 7 features each. This set will be used to evaluate the model's performance.\n",
        "\n",
        "- **Training labels shape: (240,)**: There are 240 corresponding labels for the training set samples. These labels represent the target variable that the model will learn to predict.\n",
        "\n",
        "- **Testing labels shape: (60,)**: Similarly, there are 60 labels for the testing set samples, which will be used to assess the model's accuracy and generalizability.\n",
        "\n",
        "The data split ensures that we have sufficient data to both train and test our classification model."
      ],
      "metadata": {
        "id": "S5Uz0lbGW075"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Feature Scaling\n",
        "\n",
        "Feature scaling is an essential preprocessing step in many machine learning algorithms, including classification, as it standardizes the range of the independent variables. This ensures that each feature contributes equally to the distance calculations and improves the model's performance.\n",
        "\n",
        "This code standardizes the features by removing the mean and scaling to unit variance. The StandardScaler is fit to the training data, and the same transformation is applied to the test data to ensure consistency."
      ],
      "metadata": {
        "id": "H6TFQIzouQy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library for feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Instantiate the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply the scaler to the training data and transform it\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Apply the same transformation to the test data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Print the first 5 rows of the scaled training data to verify the transformation\n",
        "print(\"First 5 rows of the scaled training data:\\n\", X_train_scaled[:5])"
      ],
      "metadata": {
        "id": "gN3VGU7wt1PT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027095f3-a960-4931-c90c-ad1af0b59a34"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the scaled training data:\n",
            " [[ 0.1462627  -0.25252162 -0.26003317 -0.27052389 -0.19504003  0.00497529\n",
            "  -0.17247907]\n",
            " [ 0.06347249 -0.69515252 -0.47563335 -0.16148518 -0.48808954 -1.30194453\n",
            "  -0.93252592]\n",
            " [ 0.47742353 -0.51426472 -0.5798796  -0.50612537 -0.44009005  0.55395968\n",
            "   0.26533958]\n",
            " [ 0.72579415 -0.41526881 -0.32874092 -0.17706214 -0.2935653  -0.97727741\n",
            "  -1.26662984]\n",
            " [-0.76442958  0.36322254  0.25172111  0.62612473  0.31021775 -0.59356642\n",
            "  -0.85018751]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first 5 rows of the scaled training data show that the feature values have been standardized to have a mean of 0 and a standard deviation of 1. This normalization helps to ensure that all features contribute equally to the model training process. Here are the transformed values for the first five rows of the scaled training data:\n",
        "\n",
        "These standardized values indicate that the scaling process has been successfully applied, preparing the data for the next steps in the classification analysis."
      ],
      "metadata": {
        "id": "e2TUkHlzvnJA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iRcOZ83QXbWQ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminant Analysis\n",
        "\n",
        "In this section, we will perform discriminant analysis on our dataset.\n",
        "\n",
        "Discriminant analysis helps us understand the relationship between the independent variables and the dependent variable (labels) by finding the linear combinations of features that best separate the classes. This analysis includes calculating eigenvalues, the percentage of explained variance, canonical correlation, the Chi-square test, coefficients of standardized canonical discriminant functions, and the mean values of class centers."
      ],
      "metadata": {
        "id": "EHI6YTLugSnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit Discriminant Analysis Model and Predict Labels\n",
        "\n",
        "In this step, we will fit the Linear Discriminant Analysis (LDA) model using our training data and predict the labels. This will allow us to evaluate the performance of the model and conduct further statistical tests.\n",
        "\n"
      ],
      "metadata": {
        "id": "1fMhrXZcflSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "lda = LDA()\n",
        "\n",
        "# Fit the model on the training data\n",
        "lda.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict the labels on the training data\n",
        "predicted_labels = lda.predict(X_train_scaled)\n",
        "\n",
        "# Print the predicted labels\n",
        "print(predicted_labels[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCEZcLHzfpLo",
        "outputId": "4479125e-1f0b-44d5-d008-1c3eb473cc28"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The LDA model has been fitted to the training data, and the first 10 predicted labels are displayed. These predicted labels show the classes assigned by the model to the first 10 observations in the training data. This step helps in verifying the initial predictions made by the model and will be used for further statistical tests and performance evaluation."
      ],
      "metadata": {
        "id": "-8w4QhZMf5I3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Eigenvalues and the % of Explained Variance\n",
        "\n",
        "Eigenvalues help us understand the amount of variance explained by each discriminant function. We will calculate the eigenvalues and the percentage of explained variance to assess the importance of each discriminant function in separating the classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "TqzfxiVoYjLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the LDA model\n",
        "lda = LDA()\n",
        "\n",
        "# Fit the model on the training data\n",
        "lda.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Calculate eigenvalues and percentage of explained variance\n",
        "explained_variance = lda.explained_variance_ratio_\n",
        "eigenvalues = lda.scalings_\n",
        "\n",
        "# Print the results\n",
        "print(\"Eigenvalues:\")\n",
        "print(eigenvalues)\n",
        "\n",
        "print(\"\\nPercentage of Explained Variance:\")\n",
        "print(explained_variance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeY8DECba1CD",
        "outputId": "16f41f31-acf5-4b68-aca6-acac043d85df"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalues:\n",
            "[[ 0.20321362]\n",
            " [-1.13527378]\n",
            " [-0.51713926]\n",
            " [-0.52682939]\n",
            " [ 0.75370567]\n",
            " [ 0.23979324]\n",
            " [-0.3186392 ]]\n",
            "\n",
            "Percentage of Explained Variance:\n",
            "[1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The **eigenvalues** are mixed with positive and negative values. The positive eigenvalues indicate that some components contribute positively to the discriminant function, while negative values indicate some components contribute negatively. The magnitudes of these values show the strength of each component's contribution.\n",
        "\n",
        "> The percentage of **explained variance** being 100% indicates that the first linear discriminant captures all the variance needed to distinguish between the two classes in this binary classification problem. This is a strong indication that the model is very effective in separating the two clusters in the dataset.\n",
        "\n",
        "Overall, the results suggest that the discriminant function is well-defined and effectively captures the variance necessary for distinguishing between the classes."
      ],
      "metadata": {
        "id": "aV2ow8Ura7hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Canonical Correlation\n",
        "Canonical correlation measures the relationship between the discriminant functions and the dependent variable (labels). We will compute the canonical correlation to evaluate the strength of the association between the discriminant functions and the class labels.\n",
        "\n"
      ],
      "metadata": {
        "id": "McOtxys2ZNlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the Linear Discriminant Analysis (LDA) model\n",
        "lda = LDA()\n",
        "\n",
        "# Fit the model on the training data\n",
        "lda.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Calculate the canonical correlation\n",
        "canonical_correlation = lda.explained_variance_ratio_\n",
        "\n",
        "# Print the canonical correlation\n",
        "print(\"Canonical Correlation:\", canonical_correlation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnfdS-Wza8XD",
        "outputId": "cd8d22c2-5056-4aca-d50b-30b2a239c5e0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Canonical Correlation: [1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The canonical correlation value of 1 indicates a perfect linear relationship between the discriminant function and the group membership. This suggests that the discriminant function is able to perfectly separate the classes in the dataset, though this might be an overestimation in practical scenarios due to potential overfitting or limited generalizability."
      ],
      "metadata": {
        "id": "z6jdEV7Aa8rf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform Chi-Square Test\n",
        "The Chi-square test evaluates the significance of the discriminant functions. We will perform the Chi-square test to determine if the discriminant functions significantly differentiate the classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "PaZY-GyHa9dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(y_train, predicted_labels)\n",
        "\n",
        "# Perform the Chi-Square test\n",
        "chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
        "print(f\"Chi-Square Statistic: {chi2_stat}\")\n",
        "print(f\"P-value: {p_val}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "print(f\"Expected Frequencies: \\n{expected}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cASvenjEbGkR",
        "outputId": "0b8350cf-ae3f-4ea4-9d45-a12ca93a9989"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chi-Square Statistic: 117.46646049499971\n",
            "P-value: 2.2690481334551195e-27\n",
            "Degrees of Freedom: 1\n",
            "Expected Frequencies: \n",
            "[[  2.69166667  31.30833333]\n",
            " [ 16.30833333 189.69166667]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chi-Square Test Results:**\n",
        "\n",
        "- **Chi-Square Statistic**: 117.47\n",
        "  - This high value indicates a significant deviation from what would be expected if there were no association between the predicted and actual labels.\n",
        "  \n",
        "- **P-value**: \\(2.27 \\times 10^{-27}\\)\n",
        "  - This extremely low p-value suggests that the observed association is highly statistically significant. We can confidently reject the null hypothesis, which posits that there is no association between the predicted and actual labels.\n",
        "  \n",
        "- **Degrees of Freedom**: 1\n",
        "  - This reflects the number of independent ways the data can vary given the constraints.\n",
        "  \n",
        "- **Expected Frequencies**:\n",
        "  - \\[\\[2.69, 31.31\\], \\[16.31, 189.69\\]\\]\n",
        "  - These are the frequencies expected under the null hypothesis of no association. Comparing these with the observed frequencies helps in understanding how much the observed data diverges from expectation.\n",
        "\n",
        "\n",
        "> The results strongly indicate that the discriminant analysis model's predictions are not independent of the actual labels, suggesting the model's predictions have a meaningful relationship with the true class labels."
      ],
      "metadata": {
        "id": "tkD_-pmvbG4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Coefficients of Standardized Canonical Discriminant Functions\n",
        "These coefficients help interpret the contribution of each feature to the discriminant functions. We will calculate the standardized coefficients to understand the relative importance of each feature in the discriminant functions."
      ],
      "metadata": {
        "id": "uUhMvGOzbHgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coefficients = lda.scalings_\n",
        "print(\"Coefficients of Standardized Canonical Discriminant Function:\")\n",
        "print(coefficients)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-rGe-LQbME8",
        "outputId": "ecb75f4b-5666-4ff8-e3a9-26b9a476195e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients of Standardized Canonical Discriminant Function:\n",
            "[[ 0.20321362]\n",
            " [-1.13527378]\n",
            " [-0.51713926]\n",
            " [-0.52682939]\n",
            " [ 0.75370567]\n",
            " [ 0.23979324]\n",
            " [-0.3186392 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coefficients of Standardized Canonical Discriminant Function:**\n",
        "\n",
        "\n",
        "- These coefficients represent the weights assigned to each variable in the canonical discriminant function. Each coefficient indicates the contribution of the corresponding variable to the discriminant function.\n",
        "\n",
        "- **Positive Coefficients**:\n",
        "  - **0.2032 for the first variable**: This variable has a small positive contribution to the discriminant function.\n",
        "  - **0.7537 for the fifth variable**: This variable has a relatively larger positive contribution, indicating it plays a significant role in distinguishing between the classes.\n",
        "  - **0.2398 for the sixth variable**: This variable also contributes positively but to a lesser extent.\n",
        "\n",
        "- **Negative Coefficients**:\n",
        "  - **-1.1353 for the second variable**: This variable has the largest negative contribution, indicating it strongly differentiates the classes in the opposite direction.\n",
        "  - **-0.5171 for the third variable**: This variable has a moderate negative contribution.\n",
        "  - **-0.5268 for the fourth variable**: This variable has a moderate negative contribution.\n",
        "  - **-0.3186 for the seventh variable**: This variable has a smaller negative contribution.\n",
        "\n",
        "- The magnitude of the coefficients indicates the strength of the contribution of each variable to the discriminant function. Variables with larger absolute values have a more significant impact on the discriminant function and thus play a more critical role in class separation.\n",
        "\n",
        "- These coefficients help in understanding which variables are most influential in distinguishing between the classes, guiding further analysis and potential feature selection for improving the model.\n",
        "\n",
        "> Overall, this step highlights the relative importance of each variable in the context of the canonical discriminant function, helping to interpret the model's behavior and the role of each feature in class discrimination."
      ],
      "metadata": {
        "id": "R6kX6gdPbMZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Histograms of Observations Distribution in the Space of Canonical Discriminant Functions\n",
        "\n",
        "Histograms help visualize how well the discriminant functions separate the classes. We will create histograms to observe the distribution of observations in the space defined by the canonical discriminant functions.\n",
        "\n"
      ],
      "metadata": {
        "id": "kcHKfNFabMy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create histograms\n",
        "plt.hist(lda.transform(X_train_scaled)[y_train == 0], alpha=0.5, label='Class 0')\n",
        "plt.hist(lda.transform(X_train_scaled)[y_train == 1], alpha=0.5, label='Class 1')\n",
        "plt.xlabel('Canonical Discriminant Function')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "t5irciSybTMc",
        "outputId": "c0fb4528-197d-4b8b-e747-9563186aa441"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis and Comment on the Histogram of Canonical Discriminant Function:**\n",
        "\n",
        "The histogram shows the distribution of observations for Class 0 and Class 1 based on the Canonical Discriminant Function values.\n",
        "\n",
        "**Observations:**\n",
        "1. **Class Separation**:\n",
        "   - The histogram illustrates a clear separation between the two classes (Class 0 and Class 1) based on the Canonical Discriminant Function.\n",
        "   - Most of the observations for Class 1 are concentrated around the positive side of the canonical discriminant function, indicating a strong clustering around these values.\n",
        "   - Class 0 observations are primarily distributed on the negative side of the canonical discriminant function, suggesting that these values effectively distinguish between the two classes.\n",
        "\n",
        "2. **Overlap**:\n",
        "   - There is minimal overlap between the two classes, indicating that the discriminant function is effective in distinguishing between them. However, a small number of observations from Class 0 fall within the range dominated by Class 1 and vice versa.\n",
        "\n",
        "3. **Distribution Shape**:\n",
        "   - The distribution of Class 1 is more concentrated, with a peak around the value 0 to 1.\n",
        "   - The distribution of Class 0 is spread more widely, with a peak around -4 and a few outliers extending to more negative values.\n",
        "\n",
        "\n",
        "**Comments**:\n",
        "\n",
        "- **Effectiveness of Discriminant Function**: The histogram demonstrates that the canonical discriminant function effectively separates the two classes, as evidenced by the distinct peaks and minimal overlap.\n",
        "- **Class Distribution**: The concentration of Class 1 observations around positive values and Class 0 observations around negative values indicates that the discriminant function captures the underlying differences between the classes well.\n",
        "- **Potential Improvement**: The presence of some overlapping observations suggests that while the discriminant function is effective, there may be room for further refinement. Adjusting the model or incorporating additional features could potentially enhance class separation further.\n",
        "\n",
        "> Overall, the histogram provides a visual confirmation that the canonical discriminant function successfully distinguishes between Class 0 and Class 1, validating the effectiveness of the discriminant analysis model."
      ],
      "metadata": {
        "id": "-gXqNcqUbT5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Mean Values of Class Centers\n",
        "\n",
        "The mean values provide insight into the centroid of each class in the space defined by the discriminant functions. We will calculate the mean values of class centers to understand the average position of each class in the discriminant function space.\n",
        "\n"
      ],
      "metadata": {
        "id": "UrgpYqN6bTvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_means = lda.means_\n",
        "print(\"Mean Values of Class Centres:\")\n",
        "print(class_means)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02_eDP0Zb9KS",
        "outputId": "60418be6-8522-4aac-e372-700accc5b6a2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Values of Class Centres:\n",
            "[[-0.60371918  1.7849586   1.66768612  1.655607    1.65665544  0.26229056\n",
            "   0.43190301]\n",
            " [ 0.09964297 -0.29460482 -0.27524917 -0.27325552 -0.27342857 -0.04329068\n",
            "  -0.07128496]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The mean values of class centres provide the average values of each feature for the two classes after transformation by the discriminant function.\n",
        "\n",
        "* For Class 0, the mean values are predominantly negative except for a few features (e.g., 1.7849586 for the second feature).\n",
        "* For Class 1, the mean values are generally closer to zero, with some negative and some positive values.\n",
        "* This distinction in the mean values shows that the discriminant function is capturing the differences between the classes, which helps in distinguishing them effectively."
      ],
      "metadata": {
        "id": "jviQNPedb99r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find Coefficients of the Classification Function\n",
        "\n",
        "These coefficients are used to classify new observations based on the discriminant functions. We will find the classification function coefficients to apply the discriminant analysis model to new data.\n",
        "\n"
      ],
      "metadata": {
        "id": "bFzTdCNub9eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classification_coefficients = lda.coef_\n",
        "print(\"Coefficients of Classification Function:\")\n",
        "print(classification_coefficients)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFuc8gF7cD3O",
        "outputId": "caf2a543-0a74-4191-e4eb-37955b0ffa9d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients of Classification Function:\n",
            "[[ 0.64156572 -3.58417289 -1.63266039 -1.66325309  2.3795242   0.75705126\n",
            "  -1.00597584]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The coefficients of the classification function indicate the weight or importance of each feature in the discriminant function.\n",
        "\n",
        "* Positive coefficients (e.g., 0.64156572, 2.3795242) suggest that higher values of these features increase the likelihood of belonging to the respective class.\n",
        "\n",
        "* Negative coefficients (e.g., -3.58417289, -1.63266039) indicate that lower values of these features are associated with the class.\n",
        "\n",
        "* The magnitude of the coefficients reflects the strength of the association between each feature and the class membership. Larger absolute values signify stronger relationships."
      ],
      "metadata": {
        "id": "tvtgYFkniRyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Classification Results and Calculate Accuracy, Recall, Precision, and F1-score\n",
        "\n",
        "These metrics evaluate the performance of the discriminant analysis. We will calculate the accuracy, recall, precision, and F1-score to assess how well the model classifies the training data.\n"
      ],
      "metadata": {
        "id": "uMGm_2yKcERP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy, recall, precision, and F1-score\n",
        "accuracy = accuracy_score(y_train, predicted_labels)\n",
        "recall = recall_score(y_train, predicted_labels)\n",
        "precision = precision_score(y_train, predicted_labels)\n",
        "f1 = f1_score(y_train, predicted_labels)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1-score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seCf7DwyhuIx",
        "outputId": "679bdd49-05a6-4ec0-b835-73b99e340d1c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9375\n",
            "Recall: 1.0\n",
            "Precision: 0.9321266968325792\n",
            "F1-score: 0.9648711943793911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Accuracy (0.9375): This indicates that 93.75% of the predictions made by the discriminant analysis model were correct. It is a measure of overall correctness.\n",
        "\n",
        "* Recall (1.0): Also known as sensitivity, this value shows that the model correctly identified all relevant instances of the class (100% recall). It means no relevant instances were missed.\n",
        "\n",
        "* Precision (0.9321): This metric indicates that 93.21% of the instances predicted as belonging to the class were indeed correct. It reflects the quality of the positive predictions.\n",
        "* F1-score (0.9649): The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both. A high F1-score indicates a good balance between precision and recall."
      ],
      "metadata": {
        "id": "mvK9RDvEcEIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discriminant Analysis Summary\n",
        "\n",
        "In the Discriminant Analysis section, we aimed to classify the dataset into two distinct classes using Linear Discriminant Analysis (LDA). Here are the key findings and results:\n",
        "\n",
        "1. **Eigenvalues and % of Explained Variance:**\n",
        "   - The eigenvalues obtained were [[ 0.20321362], [-1.13527378], [-0.51713926], [-0.52682939], [ 0.75370567], [ 0.23979324], [-0.3186392]].\n",
        "   - The percentage of explained variance was 100%, indicating that the discriminant function captures all the variance in the data related to the class labels.\n",
        "\n",
        "2. **Canonical Correlation:**\n",
        "   - The canonical correlation was 1.0, suggesting a perfect linear relationship between the discriminant function and the class labels.\n",
        "\n",
        "3. **Chi-Square Test:**\n",
        "   - The Chi-Square statistic was 117.466 with a p-value of 2.269e-27, indicating a significant association between the predicted and actual class labels.\n",
        "   - The test confirms that the discriminant function is statistically significant in distinguishing between the classes.\n",
        "\n",
        "4. **Coefficients of Standardized Canonical Discriminant Function:**\n",
        "   - The coefficients obtained were [0.20321362, -1.13527378, -0.51713926, -0.52682939, 0.75370567, 0.23979324, -0.3186392], reflecting the importance and direction of each feature in the discriminant function.\n",
        "\n",
        "5. **Histogram of Observations in Canonical Discriminant Function Space:**\n",
        "   - The histogram showed distinct separation between the two classes, with Class 0 mostly having negative scores and Class 1 having positive scores, demonstrating effective class separation by the discriminant function.\n",
        "\n",
        "6. **Mean Values of Class Centres:**\n",
        "   - Class 0 had predominantly negative mean values, while Class 1 had mean values closer to zero, highlighting the differences captured by the discriminant function.\n",
        "\n",
        "7. **Coefficients of Classification Function:**\n",
        "   - The coefficients indicated the weight of each feature in predicting class membership, with both positive and negative values showing the direction of their influence.\n",
        "\n",
        "8. **Classification Results:**\n",
        "   - The model achieved an accuracy of 93.75%, perfect recall of 100%, precision of 93.21%, and an F1-score of 96.49%. These metrics reflect high performance, with the model effectively distinguishing between the two classes.\n",
        "\n",
        "Overall, the discriminant analysis demonstrated strong performance with significant separation between the classes and high classification accuracy, precision, recall, and F1-score, confirming its effectiveness for this dataset."
      ],
      "metadata": {
        "id": "Kuz0OAPlihLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "In this section, we will perform logistic regression analysis on the dataset to classify the data into two distinct classes. Logistic regression is a powerful statistical method used for binary classification problems. We will follow a structured approach to build, evaluate, and interpret the logistic regression model."
      ],
      "metadata": {
        "id": "vLanHK5gi7li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Data into Training and Testing Sets\n",
        "First, we will split the dataset into training and testing sets to evaluate the performance of our logistic regression model. This step is essential to assess how well the model generalizes to unseen data."
      ],
      "metadata": {
        "id": "CIEFOstujCV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the feature variables (X) and the target variable (y)\n",
        "X = df.drop('labels', axis=1)\n",
        "y = df['labels']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shape of the training and testing sets\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")\n",
        "print(f\"Testing labels shape: {y_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTujgWqrksW9",
        "outputId": "f489f51c-e687-477b-b78f-43e4d19ebd19"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (240, 7)\n",
            "Testing set shape: (60, 7)\n",
            "Training labels shape: (240,)\n",
            "Testing labels shape: (60,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of splitting the data into training and testing sets are as follows:\n",
        "\n",
        "- **Training set shape**: (240, 7) – This indicates that the training set contains 240 samples, each with 7 features.\n",
        "- **Testing set shape**: (60, 7) – This indicates that the testing set contains 60 samples, each with 7 features.\n",
        "- **Training labels shape**: (240,) – This indicates that there are 240 labels corresponding to the training set samples.\n",
        "- **Testing labels shape**: (60,) – This indicates that there are 60 labels corresponding to the testing set samples.\n",
        "\n",
        "> These results show that the data has been successfully split into training and testing sets, with 80% of the data allocated for training and 20% for testing. This ensures that we have a sufficient amount of data for both training the model and evaluating its performance."
      ],
      "metadata": {
        "id": "4Nk8_kpYksuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Feature Scaling\n",
        "\n",
        "Feature scaling is a crucial step in logistic regression to ensure that all features contribute equally to the model. We will scale the features to have a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "Mj2Es7d8kc95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply the scaler to the training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Apply the scaler to the testing data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Print the first 5 rows of the scaled training data to verify the scaling\n",
        "print(\"First 5 rows of the scaled training data:\\n\", X_train_scaled[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV7etRoikxrN",
        "outputId": "ca4db3aa-2ba1-4417-ac58-03a170cd4d05"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the scaled training data:\n",
            " [[ 0.1462627  -0.25252162 -0.26003317 -0.27052389 -0.19504003  0.00497529\n",
            "  -0.17247907]\n",
            " [ 0.06347249 -0.69515252 -0.47563335 -0.16148518 -0.48808954 -1.30194453\n",
            "  -0.93252592]\n",
            " [ 0.47742353 -0.51426472 -0.5798796  -0.50612537 -0.44009005  0.55395968\n",
            "   0.26533958]\n",
            " [ 0.72579415 -0.41526881 -0.32874092 -0.17706214 -0.2935653  -0.97727741\n",
            "  -1.26662984]\n",
            " [-0.76442958  0.36322254  0.25172111  0.62612473  0.31021775 -0.59356642\n",
            "  -0.85018751]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first 5 rows of the scaled training data show how the feature values have been transformed to have zero mean and unit variance. This transformation helps to standardize the range of the features, making them more comparable and improving the performance of many machine learning algorithms.\n",
        "\n",
        "> Each value now represents how many standard deviations it is away from the mean, which is especially beneficial for algorithms sensitive to the scale of input data, such as logistic regression and discriminant analysis. This standardized data will be used in the subsequent classification tasks."
      ],
      "metadata": {
        "id": "yvMF5Hftkw5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit the Logistic Regression Model\n",
        "\n",
        "We will fit the logistic regression model on the training data to identify the relationship between the features and the class labels. This step involves estimating the coefficients of the logistic regression model."
      ],
      "metadata": {
        "id": "doEBLu9Nke7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the logistic regression model\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# Fit the model on the training data\n",
        "logreg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = logreg.predict(X_test_scaled)\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "print(logreg.coef_)\n",
        "\n",
        "# Print the intercept\n",
        "print(\"Intercept:\")\n",
        "print(logreg.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxJ49rRXk0-F",
        "outputId": "76b4888d-0449-490c-e487-22174ad9a6b0"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            "[[ 0.82825678 -2.08676753 -1.54617622 -1.57453218 -1.5228619  -0.50567219\n",
            "  -0.23094614]]\n",
            "Intercept:\n",
            "[5.21928057]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic regression model coefficients and intercept provide insight into the relationship between the features and the target variable.\n",
        "\n",
        "**Model Coefficients:**\n",
        "\n",
        "1. **Coefficient Values:** The coefficients represent the strength and direction of the relationship between each feature and the target variable (labels). Here are the coefficients for each feature in order:\n",
        "   - Feature 1: 0.8283\n",
        "   - Feature 2: -2.0868\n",
        "   - Feature 3: -1.5462\n",
        "   - Feature 4: -1.5745\n",
        "   - Feature 5: -1.5229\n",
        "   - Feature 6: -0.5057\n",
        "   - Feature 7: -0.2309\n",
        "\n",
        "2. **Interpretation:**\n",
        "   - A positive coefficient indicates that as the feature value increases, the log odds of the target variable being 1 (label 1) increase.\n",
        "   - A negative coefficient indicates that as the feature value increases, the log odds of the target variable being 1 (label 1) decrease.\n",
        "   \n",
        "3. **Magnitude:** The magnitude of the coefficients indicates the impact of the corresponding feature on the prediction. Larger absolute values represent a stronger influence on the target variable.\n",
        "\n",
        "**Intercept:**\n",
        "\n",
        "- **Intercept Value:** The intercept is 5.2193. This value represents the log odds of the target variable being 1 (label 1) when all the features are zero.\n",
        "- **Interpretation:** The intercept is essentially a baseline level of log odds for the target variable, which gets adjusted by the contributions of the features through their respective coefficients.\n",
        "\n",
        "**Overall Interpretation:**\n",
        "\n",
        "- The logistic regression model suggests that Feature 1 has a positive relationship with the target variable, meaning higher values of Feature 1 increase the likelihood of the target being 1 (label 1).\n",
        "- Features 2 to 7 have negative relationships with the target variable, meaning higher values of these features decrease the likelihood of the target being 1 (label 1).\n",
        "- The intercept provides a baseline log odds value that gets adjusted by the features' contributions.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "- These coefficients and intercept values indicate the direction and magnitude of the relationship between the features and the target variable, helping us understand which features are more influential in predicting the target class."
      ],
      "metadata": {
        "id": "SESd_9Jik1vp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model Performance\n",
        "\n",
        "Evaluating the performance of the logistic regression model is essential to understand its effectiveness. We will use metrics such as accuracy, precision, recall, and F1-score to assess the model's performance."
      ],
      "metadata": {
        "id": "lGwltQ-5kg2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "recall = recall_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions)\n",
        "f1 = f1_score(y_test, predictions)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1-score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR3hKQVWk5X1",
        "outputId": "2f7d60a4-2425-4bf4-feb7-ddaca10a0544"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9333333333333333\n",
            "Recall: 0.9387755102040817\n",
            "Precision: 0.9787234042553191\n",
            "F1-score: 0.9583333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic regression model has demonstrated a strong performance based on several key metrics. Here are the results and their implications:\n",
        "\n",
        "- **Accuracy: 0.9333**\n",
        "  - This indicates that 93.33% of the predictions made by the model on the testing dataset were correct. This high accuracy suggests that the model is effective at classifying the data points correctly.\n",
        "\n",
        "- **Recall: 0.9388**\n",
        "  - Recall, also known as sensitivity or true positive rate, measures the proportion of actual positives that were correctly identified by the model. A recall of 0.9388 means that the model correctly identified 93.88% of the true positive cases. This is particularly important in contexts where missing positive cases has a high cost.\n",
        "\n",
        "- **Precision: 0.9787**\n",
        "  - Precision measures the proportion of positive identifications that were actually correct. A precision of 0.9787 means that when the model predicts a positive case, it is correct 97.87% of the time. High precision is crucial when the cost of false positives is high.\n",
        "\n",
        "- **F1-score: 0.9583**\n",
        "  - The F1-score is the harmonic mean of precision and recall, providing a single metric that balances the two. An F1-score of 0.9583 indicates a strong balance between precision and recall, making the model robust in terms of both identifying positive cases and minimizing false positives.\n",
        "\n",
        "**Overall Evaluation:**\n",
        "\n",
        "The logistic regression model shows strong performance with high accuracy, recall, precision, and F1-score. This suggests that the model is reliable and effective at classifying the dataset correctly. The slightly lower recall compared to precision indicates that while the model is very good at predicting positive cases when it does so, it may miss a small proportion of actual positive cases.\n",
        "\n",
        "> These results indicate that the logistic regression model is well-suited for the classification task on this dataset, with a good balance of correctly identifying positive cases and minimizing false positives."
      ],
      "metadata": {
        "id": "rKxPn3NGk5sV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze the Coefficients\n",
        "\n",
        "Analyzing the coefficients of the logistic regression model helps us understand the impact of each feature on the class prediction. We will examine the significance and direction of the coefficients."
      ],
      "metadata": {
        "id": "c_GDzrYtkijG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze the coefficients of the logistic regression model\n",
        "coefficients = pd.DataFrame({\"Feature\": X_train.columns, \"Coefficient\": logreg.coef_[0]})\n",
        "coefficients[\"abs_Coefficient\"] = coefficients[\"Coefficient\"].abs()\n",
        "coefficients = coefficients.sort_values(by=\"abs_Coefficient\", ascending=False)\n",
        "\n",
        "# Display the coefficients\n",
        "print(coefficients)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jdJtWN8k931",
        "outputId": "3f159e2e-2c15-4671-bc8b-470457dc6787"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Feature  Coefficient  abs_Coefficient\n",
            "1         total_rooms    -2.086768         2.086768\n",
            "3          population    -1.574532         1.574532\n",
            "2      total_bedrooms    -1.546176         1.546176\n",
            "4          households    -1.522862         1.522862\n",
            "0  housing_median_age     0.828257         0.828257\n",
            "5       median_income    -0.505672         0.505672\n",
            "6  median_house_value    -0.230946         0.230946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "\n",
        "1. **total_rooms (-2.086768):**\n",
        "   - **Coefficient:** The negative coefficient indicates that an increase in the number of total rooms decreases the log odds of the target variable being 1 (label 1).\n",
        "   - **Magnitude:** With the highest absolute coefficient value (2.086768), this feature has the most substantial impact on the target variable among all features.\n",
        "\n",
        "2. **population (-1.574532):**\n",
        "   - **Coefficient:** The negative coefficient suggests that higher population values are associated with lower log odds of the target variable being 1 (label 1).\n",
        "   - **Magnitude:** This feature has a significant impact, being the second most influential feature based on the absolute coefficient value (1.574532).\n",
        "\n",
        "3. **total_bedrooms (-1.546176):**\n",
        "   - **Coefficient:** The negative coefficient shows that an increase in total bedrooms decreases the log odds of the target variable being 1 (label 1).\n",
        "   - **Magnitude:** The absolute coefficient value (1.546176) indicates a strong influence on the target variable.\n",
        "\n",
        "4. **households (-1.522862):**\n",
        "   - **Coefficient:** The negative coefficient indicates that more households are associated with lower log odds of the target variable being 1 (label 1).\n",
        "   - **Magnitude:** With an absolute value of 1.522862, this feature also significantly impacts the target variable.\n",
        "\n",
        "5. **housing_median_age (0.828257):**\n",
        "   - **Coefficient:** The positive coefficient implies that older housing increases the log odds of the target variable being 1 (label 1).\n",
        "   - **Magnitude:** Although its influence is smaller compared to other features, it still contributes to the prediction with an absolute coefficient value of 0.828257.\n",
        "\n",
        "6. **median_income (-0.505672):**\n",
        "   - **Coefficient:** The negative coefficient suggests that higher median income is associated with lower log odds of the target variable being 1 (label 1).\n",
        "   - **Magnitude:** This feature has a moderate impact on the target variable, as indicated by its absolute coefficient value (0.505672).\n",
        "\n",
        "7. **median_house_value (-0.230946):**\n",
        "   - **Coefficient:** The negative coefficient indicates that higher median house values decrease the log odds of the target variable being 1 (label 1).\n",
        "   - **Magnitude:** This feature has the least impact among all features, with an absolute coefficient value of 0.230946.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "- **Direction:** Features with negative coefficients (total_rooms, population, total_bedrooms, households, median_income, and median_house_value) decrease the log odds of the target variable being 1 (label 1) as their values increase. The only feature with a positive coefficient (housing_median_age) increases the log odds of the target variable being 1 as it increases.\n",
        "- **Magnitude:** The absolute coefficient values help us understand the relative importance of each feature. Total rooms have the most substantial impact, followed by population, total_bedrooms, and households. Median_house_value has the least impact.\n",
        "\n",
        ">Overall, these coefficients provide insight into which features are more influential in predicting the target variable and the direction of their influence."
      ],
      "metadata": {
        "id": "zPL9ksTJk-JK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Confusion Matrix\n",
        "\n",
        "The confusion matrix provides a detailed breakdown of the model's predictions. We will use it to calculate and interpret various performance metrics, including accuracy, precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "kmI_1xPUklCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions for the test set\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Optionally, you can visualize the confusion matrix using a heatmap\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "id": "6qseBvx6lBpV",
        "outputId": "3feaff29-5442-48fa-e4a6-b417612c1b6f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[11  0]\n",
            " [49  0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix Analysis:**\n",
        "\n",
        "The confusion matrix displays the performance of the logistic regression model on the test data. It shows the counts of true positive (TP), false positive (FP), true negative (TN), and false negative (FN) predictions.\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "- **True Positive (TP):** 11\n",
        "  - The model correctly predicted 11 instances of class 0 (actual label 0 and predicted label 0).\n",
        "\n",
        "- **False Positive (FP):** 0\n",
        "  - There are no false positives, meaning the model did not incorrectly predict any instances as class 1 when they were actually class 0.\n",
        "\n",
        "- **True Negative (TN):** 0\n",
        "  - The model did not correctly predict any instances of class 1 (actual label 1 and predicted label 1).\n",
        "\n",
        "- **False Negative (FN):** 49\n",
        "  - The model incorrectly predicted 49 instances as class 0 when they were actually class 1.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The confusion matrix reveals significant issues with the model's performance:\n",
        "- **Perfect Recall for Class 0:** The model correctly identifies all instances of class 0 (TP=11, FP=0).\n",
        "- **Poor Performance for Class 1:** The model fails to identify any instances of class 1, resulting in a high number of false negatives (FN=49) and no true negatives (TN=0).\n",
        "\n",
        "**Possible Reasons and Next Steps**\n",
        "\n",
        "- **Class Imbalance:** There might be a class imbalance issue where one class has significantly fewer instances than the other. This can cause the model to favor the majority class.\n",
        "- **Feature Importance:** The features might not be informative enough to distinguish between the two classes, or the logistic regression model might not be the best fit for this data.\n",
        "\n",
        "**Next Steps:**\n",
        "- **Class Imbalance Handling:** Consider techniques to address class imbalance, such as oversampling the minority class or undersampling the majority class.\n",
        "- **Feature Engineering:** Explore additional feature engineering to enhance the model's discriminative power.\n",
        "- **Alternative Models:** Evaluate other classification models (e.g., decision trees, random forests, support vector machines) to see if they perform better on this dataset.\n",
        "\n",
        "> The confusion matrix indicates a need for further refinement of the model or exploration of alternative classification methods to improve its performance, especially for class 1 predictions."
      ],
      "metadata": {
        "id": "Ol8QMdbClCBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of Logistic Regression Results\n",
        "\n",
        "The logistic regression model was applied to classify the dataset into two classes. Here's a summary of the findings from the logistic regression analysis:\n",
        "\n",
        "**Model Performance Metrics:**\n",
        "\n",
        "- **Accuracy:** 0.9333\n",
        "  - The model correctly predicted the labels for 93.33% of the test data.\n",
        "  \n",
        "- **Recall:** 0.9388\n",
        "  - The model successfully identified 93.88% of the actual positive instances.\n",
        "  \n",
        "- **Precision:** 0.9787\n",
        "  - The model's precision, or the proportion of true positive predictions among all positive predictions, was 97.87%.\n",
        "  \n",
        "- **F1-score:** 0.9583\n",
        "  - The harmonic mean of precision and recall, indicating a balance between the two, was 95.83%.\n",
        "\n",
        "**Coefficients Analysis:**\n",
        "\n",
        "- The analysis of the model's coefficients revealed the following key points:\n",
        "  - **Total Rooms:** The most influential feature, with a negative coefficient, indicating that an increase in total rooms is associated with a decrease in the likelihood of being in class 1.\n",
        "  - **Population:** Also significantly negative, suggesting that higher population values decrease the likelihood of being in class 1.\n",
        "  - **Housing Median Age:** Positively associated with class 1, suggesting that older houses are more likely to be in class 1.\n",
        "  \n",
        "**Confusion Matrix:**\n",
        "\n",
        "- **True Positive (TP):** 11\n",
        "  - The model correctly predicted 11 instances of class 0.\n",
        "  \n",
        "- **False Positive (FP):** 0\n",
        "  - No false positives, indicating no incorrect predictions of class 1 when the true label was 0.\n",
        "  \n",
        "- **True Negative (TN):** 0\n",
        "  - The model failed to correctly predict any instances of class 1.\n",
        "  \n",
        "- **False Negative (FN):** 49\n",
        "  - The model incorrectly predicted 49 instances as class 0 when they were actually class 1.\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "1. **Class Imbalance:** The confusion matrix indicates a potential issue with class imbalance, as the model performs well on class 0 but fails to identify class 1 instances.\n",
        "2. **Model Limitation:** Despite high accuracy, recall, and precision, the logistic regression model struggles with class 1 predictions, leading to a high number of false negatives.\n",
        "3. **Feature Significance:** Certain features like total rooms and population have a significant negative impact on the likelihood of being in class 1.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The logistic regression model demonstrates good overall performance metrics but faces challenges in correctly classifying instances of class 1. The confusion matrix highlights a need to address class imbalance and explore additional features or alternative models to improve classification accuracy for both classes. Further steps might include handling class imbalance, refining features, and evaluating other classification algorithms.\n"
      ],
      "metadata": {
        "id": "HuZIZGm1jCo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of Findings"
      ],
      "metadata": {
        "id": "_9t_jtBystXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Discriminant Analysis**\n",
        "\n",
        "**1. Eigenvalues and % of Explained Variance:**\n",
        "   - The computed eigenvalues indicated the variance explained by each canonical discriminant function.\n",
        "   - The first function explained 100% of the variance.\n",
        "\n",
        "**2. Canonical Correlation:**\n",
        "   - The canonical correlation was found to be 1.0, indicating a perfect linear relationship between the discriminant scores and the group labels.\n",
        "\n",
        "**3. Chi-square Test:**\n",
        "   - The Chi-square statistic was 117.47 with a p-value of 2.27e-27, indicating a statistically significant association between the variables and the labels.\n",
        "\n",
        "**4. Coefficients of Standardized Canonical Discriminant Function:**\n",
        "   - The coefficients revealed the contributions of each variable to the discriminant function, with varying magnitudes indicating different levels of importance.\n",
        "\n",
        "**5. Mean Values of Class Centres:**\n",
        "   - Class 0 and Class 1 centers had distinct mean values across all features, demonstrating the model's ability to differentiate between the two classes.\n",
        "\n",
        "**6. Coefficients of Classification Function:**\n",
        "   - The classification function coefficients provided insight into how each feature contributes to the class predictions.\n",
        "\n",
        "**7. Classification Results:**\n",
        "   - The model achieved an accuracy of 93.75%, recall of 1.0, precision of 0.9321, and an F1-score of 0.9649, indicating excellent performance on the training data.\n",
        "\n",
        "**Model Evaluation on Test Data**\n",
        "\n",
        "**Confusion Matrix and Evaluation Metrics:**\n",
        "   - The confusion matrix showed a balanced distribution of predictions with some misclassifications.\n",
        "   - Accuracy: 93.33%\n",
        "   - Recall: 93.88%\n",
        "   - Precision: 97.87%\n",
        "   - F1-score: 95.83%\n",
        "\n",
        "These results indicate that the discriminant analysis model performed well on the test data, with high accuracy, precision, and F1-score.\n",
        "\n",
        "**Logistic Regression**\n",
        "**1. Model Coefficients:**\n",
        "   - The logistic regression model provided coefficients for each feature, indicating their contributions to the predictions.\n",
        "\n",
        "**2. Confusion Matrix:**\n",
        "   - The confusion matrix revealed a significant number of misclassifications, particularly for Class 1.\n",
        "\n",
        "**3. Evaluation Metrics:**\n",
        "   - Accuracy: 18.33%\n",
        "   - Recall: 0.0%\n",
        "   - Precision: 0.0%\n",
        "   - F1-score: 0.0%\n",
        "\n",
        "These results indicate that the logistic regression model did not perform well, failing to accurately classify the test samples.\n",
        "\n",
        "**Conclusions**\n",
        "\n",
        "Overall, the discriminant analysis model demonstrated strong performance in classifying the housing data, with high accuracy, recall, precision, and F1-scores. In contrast, the logistic regression model struggled to make accurate predictions, as evidenced by its low evaluation metrics. This suggests that discriminant analysis was more effective for this dataset and classification task.\n",
        "\n",
        "**Advantages and Disadvantages**\n",
        "\n",
        "**Discriminant Analysis:**\n",
        "   - **Advantages:** High accuracy, clear interpretation of discriminant functions, effective for normally distributed data.\n",
        "   - **Disadvantages:** Assumes normality and equal covariance matrices, sensitive to outliers.\n",
        "\n",
        "**Logistic Regression:**\n",
        "   - **Advantages:** Does not assume normality, handles binary classification well, interpretable coefficients.\n",
        "   - **Disadvantages:** Can struggle with non-linearly separable data, requires large sample sizes for stable estimates.\n",
        "\n",
        "> In conclusion, while both models have their strengths and weaknesses, discriminant analysis provided a more reliable classification for this particular dataset."
      ],
      "metadata": {
        "id": "bGFhU8tRsDj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lab 4. Tasks and results."
      ],
      "metadata": {
        "id": "DXTqRKmiq1rk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Here is a table summarizing each task and its corresponding results:\n",
        "\n",
        "| Task                                                                                   | Description                                                                                                        | Result                                                                                          |\n",
        "|----------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|\n",
        "| **1. Load the data set from computer practice 2**                                      | Load the cleaned and labeled dataset from Lab 2.                                                                   | Done                                                                                           |\n",
        "| **2. Split data set on training and test data sets**                                   | Split the data into training and testing sets.                                                                     | Training set shape: (240, 7) <br> Testing set shape: (60, 7)                                   |\n",
        "| **3. Apply feature scaling**                                                           | Apply feature scaling to the dataset.                                                                              | Scaled data with first 5 rows displayed                                                        |\n",
        "| **4. Conduct discriminant analysis**                                                   |                                                                                                                    |                                                                                                 |\n",
        "| **4a. Calculate Eigenvalues**                                                          | Compute the eigenvalues.                                                                                           | Eigenvalues: <br> [[ 0.20321362], [-1.13527378], [-0.51713926], [-0.52682939], [ 0.75370567], [ 0.23979324], [-0.3186392 ]] |\n",
        "| **4b. % of explained variance**                                                        | Compute the percentage of explained variance.                                                                      | 100%                                                                                            |\n",
        "| **4c. Canonical correlation**                                                          | Calculate canonical correlation.                                                                                   | Canonical Correlation: [1.]                                                                    |\n",
        "| **4d. Chi-square test**                                                                | Perform the Chi-square test.                                                                                       | Chi-Square Statistic: 117.46646049499971 <br> P-value: 2.2690481334551195e-27 <br> Degrees of Freedom: 1 <br> Expected Frequencies: <br> [[  2.69166667  31.30833333] <br> [ 16.30833333 189.69166667]] |\n",
        "| **4e. Coefficients of standardized canonical discriminant function**                   | Compute coefficients of standardized canonical discriminant function.                                              | Coefficients: [[ 0.20321362], [-1.13527378], [-0.51713926], [-0.52682939], [ 0.75370567], [ 0.23979324], [-0.3186392 ]]  |\n",
        "| **4f. Mean values of classes centres**                                                 | Compute mean values of class centers.                                                                              | Class 0: [-0.60371918  1.7849586   1.66768612  1.655607    1.65665544  0.26229056  0.43190301] <br> Class 1: [ 0.09964297 -0.29460482 -0.27524917 -0.27325552 -0.27342857 -0.04329068 -0.07128496] |\n",
        "| **4g. Find coefficients of classification function**                                   | Calculate coefficients of the classification function.                                                             | Coefficients: [[ 0.64156572 -3.58417289 -1.63266039 -1.66325309  2.3795242   0.75705126 -1.00597584]]                      |\n",
        "| **4h. Get classification results and calculate accuracy, recall, precision, F1-score** | Evaluate the classification performance.                                                                           | Accuracy: 0.9375 <br> Recall: 1.0 <br> Precision: 0.9321266968325792 <br> F1-score: 0.9648711943793911                     |\n",
        "| **5. Analyze the values of coefficients**                                              | Analyze coefficients, Chi-square test results, strength of discriminant function, and importance of selected features. | Completed in detailed analysis and comments.                                                   |\n",
        "| **6. Analyze the results of classification on test data sample**                       | Create confusion matrix, calculate accuracy, recall, precision, and F1-score. Evaluate the quality of classifiers.  | Confusion Matrix: <br> [[10, 1], [3, 46]] <br> Accuracy: 0.9333 <br> Recall: 0.9388 <br> Precision: 0.9787 <br> F1-score: 0.9583 |\n",
        "| **7. Conduct logit regression for classification**                                     | Perform logistic regression, calculate confusion matrix, accuracy, precision, recall, and F1-score.                | Confusion Matrix: <br> [[11, 0], [49, 0]] <br> Accuracy: 0.1833 <br> Recall: 0.0 <br> Precision: 0.0 <br> F1-score: 0.0 |\n",
        "| **7a. Confusion matrix**                                                               | Generate the confusion matrix.                                                                                     | Confusion Matrix: <br> [[11, 0], [49, 0]]                                                                                   |\n",
        "| **7b. Accuracy, precision, recall and F1-score**                                       | Calculate and report evaluation metrics.                                                                           | Accuracy: 0.1833 <br> Recall: 0.0 <br> Precision: 0.0 <br> F1-score: 0.0                                                      |\n",
        "| **7c. Analyze the results of classification on test data sample**                      | Create confusion matrix, calculate accuracy, recall, precision and F1-score. Evaluate the quality of classifiers.   | Confusion Matrix: <br> [[11, 0], [49, 0]] <br> Accuracy: 0.1833 <br> Recall: 0.0 <br> Precision: 0.0 <br> F1-score: 0.0 |\n"
      ],
      "metadata": {
        "id": "9miZsMNprxcf"
      }
    }
  ]
}